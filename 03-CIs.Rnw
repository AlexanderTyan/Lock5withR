\Sexpr{set_parent('Lock5withR.Rnw')}

\setcounter{chapter}{2}
\Chapter{Confidence Intervals}
%how to keep samples from changing? set.seed?
%take note of skipped examples

\section{Sampling Distributions}

The key idea in this chapter is the notion of a sampling distribution.  Do not confuse it with 
the population (what we would like to know about) or the sample (what we actually have data about).
If we could repeatedly sample from a population, and if we computed a statistic from each sample,
the distribution of those statistics would be the sampling distribution.  Sampling distributions
tell us how things vary from sample to sample and are the key to interpreting data.

\subsection*{Population Parameters and Sample Statistics}
%Example 3.1
\subsection*{Sample Statistics as Point Estimates of Population Parameters}
%Example 3.2
%Example 3.3
\subsection*{Variability of Sample Statistics}

\subsubsection*{Example 3.4}
<<Example3.4>>=
head(StatisticsPhD)
mean(~FTGradEnrollment, data=StatisticsPhD) # mean enrollment in original population
@

\subsubsection*{Example 3.5}
To select a random sample of a certain size in \R, we can use the \function{sample()} function.
<<Example3.5>>=
sample10 = sample(StatisticsPhD, 10); sample10
x.bar = mean(~FTGradEnrollment, data=sample10); x.bar  # mean enrollment in sample10
@
Note that this sample has been assigned a name to which we can refer back to find the mean of that particular sample.

<<>>=
mean(~FTGradEnrollment, data=sample(StatisticsPhD, 10))  # mean enrollment in another sample 
@

<<>>=
# Now we'll do it 1000 times
sampledist <- do(1000) * mean(~FTGradEnrollment, data=sample(StatisticsPhD, 10))
@

We should check that that our sample distribution has an appropriate shape:
<<fig.width=5, fig.height=2>>=
dotPlot(~result, data=sampledist, n=50)
@

In many (but not all) situations, the sampling distribution is 
\begin{itemize}
  \item unimodal,
  \item symmetric, and
	\item bell-shaped
\end{itemize}
(The technical phrase is ``approximately normal''.)  In these situations, a 95\% confidence interval can be estimated 
with
\[ 
\mbox{statistic} \pm 2 SE
\]

\subsubsection*{Example 3.6}

This time we don't have data, but instead we have a summary of the data. We can however, still simulate the sample distribution by using the \function{rflip()} function.
%start of bootstrapping???
<<fig.width=4, fig.height=3>>=
sampledistdeg <- do(1000) * rflip(200, 0.275) # 1000 samples, each of size 200 and proportion 0.275
head(sampledistdeg, 3)
dotPlot(~ prop, width=.005, data=sampledistdeg)
@

\subsection*{Measuring Sampling Variability: The Standard Error}

\begin{boxedText}
  The standard deviation of a sampling distribution is called the \textbf{standard error}, 
	denoted $SE$.
\end{boxedText}

The standard error is our primary way of measuring how much variability there is from sample statistic
to sample statistic, and therefore how precise our estimates are.

\subsubsection*{Example 3.7}
Calculating the SE is the same as calculating the standard deviation of a sampling distribution, so we use \function{sd()}.

<<standarderror>>=
SE <- sd(~result, data=sampledist); SE    # Bootstrap from Example 3.5
SE2 <- sd(~prop, data=sampledistdeg); SE2     # Boot.Deg from Example 3.6
@

%Example 3.8
%<<>>=
%MoE <- 2 * SE; MoE                            # margin of error for 95% CI
% x.bar - MoE                                   # lower limit of 95% CI
% x.bar + MoE                                   # upper limit of 95% CI
% @

\subsection*{The Importance of Sample Size}
\subsubsection*{Example 3.9}

<<bootstrap-samplesize, fig.width=3, out.width='.3\\textwidth',>>=
sampledist.1000 <- do(1000) * rflip(1000, 0.275) # 1000 samples, each of size 1000 and proportion 0.275
sampledist.200 <- do(1000) * rflip(200, 0.275)   # 1000 samples, each of size 200 and proportion 0.275
sampledist.50 <- do(1000) * rflip(50, 0.275)     # 1000 samples, each of size 50 and proportion 0.275

dotPlot(~ prop, width=.005, xlim=c(0.05, 0.5), data=sampledist.1000)
dotPlot(~ prop, width=.005, xlim=c(0.05, 0.5), data=sampledist.200)
dotPlot(~ prop, width=.005, xlim=c(0.05, 0.5), data=sampledist.50)
@

%Example 3.10 (do confidence intervals?)

\subsection*{Importance of Random Sampling}

%Example 3.11

\section{Understanding and Interpreting Confidence Intervals}

\subsection*{Interval Estimates and Margin of Error}

\begin{boxedText}
An \term{interval estimate} 
gives a range of plausible values for a population parameter.  
\end{boxedText}

This is better than a single number (also called a point estimate) because it gives some 
indication of the precision of the estimate.

One way to express an interval estimate is with a point estimate and a \term{margin of error}.

We can convert margin of error into an interval by adding and subtracting the margin of error to/from
the statistic.

\subsubsection*{Example 3.12}

\[
0.42 \pm 0.03 \mbox{ which is the same as } (0.39, 0.45)
\]


\subsubsection*{Example 3.13}

<<marginoferror, tidy=FALSE>>=
p.hat = 0.54                        # sample proportion
MoE = 0.02                          # margin of error
p.hat - MoE                         # lower limit of interval estimate
p.hat + MoE                         # upper limit of interval estimate
@

<<>>=
p.hat =0.54 
MoE=0.10
p.hat - MoE
p.hat + MoE
@

\subsection*{Confidence Intervals}

\begin{boxedText}
A confidence interval for a parameter is an interval computed from sample data
by a method that will capture the parameter for a specified proportion of all
samples
\end{boxedText}

\begin{enumerate}
	\item The probability of correctly containing the parameter is called the coverage rate or \term{confidence level}.
	\item So 95\% of 95\% confidence intervals contain the parameter being estimated.
	\item The margins of error in the tables above were designed to produce 95\% confidence intervals.
\end{enumerate}

\subsubsection*{Example 3.14}

<<confidenceinterval, tidy=FALSE>>=
x.bar = 61.5               # given sample mean
SE = 11                    # given estimated standard error
MoE = 2 * SE; MoE          # margin of error for 95% CI
x.bar - MoE                # lower limit of 95% CI
x.bar + MoE                # upper limit of 95% CI
@

\subsection*{Understanding Confidence Intervals}
\subsubsection*{Example 3.15}

<<confidenceinterval2>>=
SE=0.03
p1=0.26 
p2=0.32
p3=0.20
MoE=2*SE
@

<<>>=
p1-MoE
p1+MoE
p2-MoE
p2+MoE
p3-MoE
p3+MoE
@

<<fig.width=4, fig.height=3>>=
p= 0.275
SE= 0.03
MoE = 2 * SE
p - MoE
p + MoE

dotPlot(~ prop, width=.005, groups = (0.215 <= prop & prop <= 0.335), data=sampledistdeg)
@

%how to do plots on page 184?

\subsection*{Interpreting Confidence Intervals}

\subsubsection*{Figure 3.13}
We can create the data needed for plots like Figure 3.13 using \function{CIsim()}.  The plot
itself uses \function{xYplot()} from the \pkg{Hmisc} package.
<<message=FALSE, seed=1234, fig.width=8>>=
results <- CIsim(200, samples=100, rdist=rbinom, args=list(size=1, prob=0.275), 
                 method=binom.test, method.args=list(success=1), verbose=FALSE, estimand=0.275)
require(Hmisc)
xYplot(Cbind(estimate,lower,upper) ~ sample,
  data=results,
  par.settings=col.mosaic(), 
  groups=cover)
@

\subsubsection*{Example 3.16}

<<>>=
x.bar= 27.655
SE= 0.009
MoE = 2 * SE
x.bar - MoE
x.bar + MoE
@

\subsubsection*{Example 3.17}

<<>>=
diff.x= -1.915
SE= 0.016
MoE = 2 * SE
diff.x - MoE
diff.x + MoE
@

\section{Constructing Bootstrap Confidence Intervals}

Here's the clever idea:  We don't have the population, but we have a sample.  Probably the sample it similar to the population in many ways.  So let's sample from our sample.  We'll call it \textbf{resampling} (also called \textbf{bootstrapping}). We want samples the same size as our original sample, so we will need to sample with replacement.  This means that we may pick some members of the population more than once and others not at all.  We'll do this many times, however, so each member of our sample will get its fair share. (Notice the similarity to and difference from sampling from populations in the previous sections.) 

\subsubsection*{Commuting in Atlanta}
<<fig.width=5, fig.height=2>>=
head(CommuteAtlanta, 3)
dotPlot(~Time, width=1, cex=.5, data=CommuteAtlanta)
mean(~Time, data=CommuteAtlanta)
sd(~Time, data=CommuteAtlanta)
@

\subsection*{Bootstrap Samples}
The computer can easily do all of the resampling by using the \function{resample()}.
<<>>=
resample(CommuteAtlanta, 10)
@

\subsection*{Bootstrap Distribution}

The example below uses data from 500 Atlanta commuters.
<<>>=
mean( ~Time, data=resample(CommuteAtlanta) )  # mean commute time in one resample
mean( ~Time, data=resample(CommuteAtlanta) )  # mean commute time in another resample

# Now we'll do it 1000 times
Bootstrap <- do(1000) * mean( ~Time, data=resample(CommuteAtlanta)) 
@

We should check that that our bootstrap distribution has an appropriate shape:
<<fig.width=5, fig.height=2>>=
dotPlot(~result, data=Bootstrap, n=50)
@

\subsubsection*{Example 3.19}
<<fig.width=4, fig.height=3>>=
BootP <- do(1000) * rflip(100, .52)
head(BootP, 3)
dotPlot(~ prop, width=.01, data=BootP)
@

\subsubsection*{Example 3.20}
Variables can be created in \R\ using the \function{c()} function then collected into
a data frame using the \function{data.frame()} function.
\authNote{The bootstrapping is still to be done here.}
<<>>=
Laughter <- data.frame( NumLaughs =  c(16, 22, 9, 31, 6, 42) )
mean( ~ NumLaughs, data=Laughter )
@

\subsection*{Estimating Standard Error Based on a Bootstrap Distribution}
\subsubsection*{Example 3.21}
Since the shape of the bootstrap distribution from Example 3.19 looks good, we can estimate the standard error.
<<>>=
SE = sd(~prop, data=BootP); SE
@

\subsubsection*{Example 3.22}
We can again use the standard error to compute a 95\% confidence interval.
<<tidy=FALSE>>=
x.bar <- mean(~Time, data = CommuteAtlanta); x.bar
SE <- sd(~result, data = Bootstrap ); SE        # standard error
MoE <- 2 * SE; MoE                              # margin of error for 95% CI
x.bar - MoE                                     # lower limit of 95% CI
x.bar + MoE                                     # upper limit of 95% CI
@

<<>>=
p.hat = 0.52
SE = sd( ~prop, data=BootP); SE      
MoE = 2 * SE; MoE                          
p.hat - MoE                                 
p.hat + MoE                                 
@
The same steps used in this example, get used in a wide variety of confidence interval situations.
\begin{enumerate}
  \item 
		Compute the statistic from the original sample.
	\item
		Create a bootstrap distribution by resampling from the sample.
		\begin{enumerate}
			\item same size samples as the original sample
			\item with replacement
			\item compute the statistic for each sample
		\end{enumerate}
		The distribution of these statistics is the bootstrap distribution
	\item
		Estimate the standard error $SE$ by computing the standard deviation of the bootstrap distribution.
	\item
		95\% CI is \[ \mbox{statistic} \pm 2 SE \]
\end{enumerate}
\section{Bootstrap Confidence Intervals Using Percentiles}
\subsection*{Confidence Intervals Based on Bootstrap Percentiles}
\subsubsection*{Example 3.23}

Another way to create a 95\% confidence interval is to use the middle 95\% of the bootstrap distribution.
The \function{cdata()} function can compute this for us as follows:
<<fig.width=5>>=
cdata(0.95, result, data=Bootstrap)
dotPlot(~ result, width=.2, groups = (27.47 <= result & result <= 31.00), data=Bootstrap)
@
This is not exactly the same as the interval of the original sample, but it is pretty close. Notice the \argument{groups=} for marking the confidence interval.

\subsubsection*{Example 3.24}

One advantage of this method is that it is easy to change the confidence level. 

To make a 90\% confidence interval, we use the middle 90\% of the sample distribution instead.
<<>>=
cdata(0.99, result, data=Bootstrap)
dotPlot(~ result, width=.2, groups = (26.98 <= result & result <= 31.58), data=Bootstrap)

cdata(0.90, result, data=Bootstrap)
dotPlot(~ result, width=.2, groups = (27.63 <= result & result <= 31.66), data=Bootstrap)
@
Notice that this interval is narrower.  This will always be the case.  Higher levels of confidence 
lead to wider confidence intervals.

(Method 1 can also be adjusted for other confidence levels as well -- 
the number 2 needs to be replaced by an appropriate alternative.)

\subsection*{Finding Confidence Intervals for Many Different Parameters}
\subsubsection*{Example 3.25}
<<fig.width=4, fig.height=3>>=
head(ExerciseHours)
bwplot(Gender~Exercise, data=ExerciseHours)
favstats(~Exercise|Gender, data=ExerciseHours)
@

<<>>=
stat <-diff(mean(Exercise~Gender, data=ExerciseHours)); stat
@

<<>>=
BootE <- do(1000) * diff(mean(Exercise~Gender, data=resample(ExerciseHours)))
head(BootE, 3)
@
<<fig.width=4, fig.height=3>>=
cdata(0.95, M, data=BootE)
dotPlot(~M, width=.25, cex=.5, groups =(-1.44 <= M & M <= 7.534), 
        xlab="Difference in mean", data=BootE)
@
<<>>=
SE <- sd( ~ M, data=BootE); SE
stat - 2 * SE
stat + 2 * SE
@

\subsubsection*{Example 3.26}
<<fig.width=4, fig.height=3>>=
head(MustangPrice, 3)
xyplot(Price~Miles, ylab="Price ($1000s)", xlab="Miles (1000s)", data=MustangPrice)
cor(Price~Miles, data=MustangPrice)
@
%correlation based on ordered pairs?
<<>>=
BootM <- do(5000) * cor(Price~Miles, data=resample((MustangPrice)))
head(BootM, 3)
@
<<fig.width=4, fig.height=3>>=
cdata(0.98, result, data=BootM)
dotPlot(~result, width=.005, groups =(-.9394 <= result & result <= -.7060), xlab="r", data=BootM)
@

\subsection*{Another Look at the Effect of Sample Size}
\subsubsection*{Example 3.27}
<<fig.width=5, fig.height=3>>=
BootP400 <- do(1000) * rflip(400, .52)
head(BootP400, 3)
cdata(0.95, prop, data=BootP400)
dotPlot(~ prop, width=.005, groups= (0.47 <= prop & prop<= 0.5675), data=BootP400)
@

\subsection*{One Caution on Constructing Bootstrap Confidence Intervals}
\subsubsection*{Example 3.28}
<<>>=
median(~ Price, data=MustangPrice )
Boot.Mustang <- do(5000) * median( ~Price, data=resample(MustangPrice) )
head(Boot.Mustang, 3)
histogram( ~ result, data=Boot.Mustang, n=50 )
@

This time the histogram does not have the desired shape.  There are two problems:
\begin{enumerate}
  \item
		The distribution is not symmetric.  (It is right skewed.)
	\item
		The distribution has spikes and gaps.

Since the median must be
an element of the sample when the sample size is 25, there are only 25 possible values for the median (and some of 
these are \emph{very} unlikely.
\end{enumerate}
Since the bootstrap distribution does not look like a normal distribution (bell-shaped, symmetric), we cannot safely use 
our methods for creating a confidence interval.