\Sexpr{set_parent('Lock5withR.Rnw')}

\Chapter{Confidence Intervals}

\section{Sampling Distributions}

The key idea in this chapter is the notion of a sampling distribution.  Do not confuse it with 
the population (what we would like to know about) or the sample (what we actually have data about).
If we could repeatedly sample from a population, and if we computed a statistic from each sample,
the distribution of those statistics would be the sampling distribution.  Sampling distributions
tell us how things vary from sample to sample and are the key to interpreting data.

\subsection*{Population Parameters and Sample Statistics}
%Example 3.1
\subsection*{Sample Statistics as Point Estimates of Population Parameters}
%Example 3.2
%Example 3.3
\subsection*{Variability of Sample Statistics}

\subsubsection*{Example 3.4}
<<Example3.4>>=
head(StatisticsPhD)
mean(~FTGradEnrollment, data=StatisticsPhD) # mean enrollment in original population
@

\subsubsection*{Example 3.5}
To select a random sample of a certain size in \R, we can use the \function{sample()} function.
<<Example3.5>>=
sample10 = sample(StatisticsPhD, 10); sample10
x.bar = mean(~FTGradEnrollment, data=sample10); x.bar  # mean enrollment in sample10
@
Note that this sample has been assigned a name to which we can refer back to find the mean of that particular sample.

<<>>=
mean(~FTGradEnrollment, data=sample(StatisticsPhD, 10))  # mean enrollment in another sample 

# Now we'll do it 1000 times
sampledist <- do(1000) * mean(~FTGradEnrollment, data=sample(StatisticsPhD, 10))
@

We should check that that our sample distribution has an appropriate shape:
<<>>=
histogram(~result, data=sampledist, n=30)
dotPlot(~result, data=sampledist, n=50)
@
%these plots are incorrect in pdf
\subsubsection*{Example 3.6}

This time we don't have data, but instead we have a summary of the data. We can however, still simulate the sample distribution by using the \function{rflip()} function.
%start of bootstrapping???
<<>>=
sampledistdeg <- do(1000) * rflip(200, 0.275) # 1000 samples, each of size 200 and proportion 0.275
head(sampledistdeg, 3)
dotPlot(~ prop, width=.005, data=sampledistdeg)
@

\subsection*{Measuring Sampling Variability: The Standard Error}

\begin{boxedText}
  The standard deviation of a sampling distribution is called the \textbf{standard error}, 
	denoted $SE$.
\end{boxedText}

The standard error is our primary way of measuring how much variability there is from sample statistic
to sample statistic, and therefore how precise our estimates are.

\subsubsection*{Example 3.7}
Calculating the SE is the same as calculating the standard deviation of a sampling distribution, so we use \function{sd()}.

<<standarderror>>=
SE <- sd(~result, data=sampledist); SE    # Bootstrap from Example 3.5
SE2 <- sd(~prop, data=sampledistdeg); SE2     # Boot.Deg from Example 3.6
@

%Example 3.8
%<<>>=
%MoE <- 2 * SE; MoE                            # margin of error for 95% CI
% x.bar - MoE                                   # lower limit of 95% CI
% x.bar + MoE                                   # upper limit of 95% CI
% @

\subsection*{The Importance of Sample Size}
\subsubsection*{Example 3.9}

<<bootstrap-samplesize, fig.width=3, out.width='.3\\textwidth',>>=
sampledist.1000 <- do(1000) * rflip(1000, 0.275) # 1000 samples, each of size 1000 and proportion 0.275
sampledist.200 <- do(1000) * rflip(200, 0.275)   # 1000 samples, each of size 200 and proportion 0.275
sampledist.50 <- do(1000) * rflip(50, 0.275)     # 1000 samples, each of size 50 and proportion 0.275

dotPlot(~ prop, width=.005, xlim=c(0.05, 0.5), data=sampledist.1000)
dotPlot(~ prop, width=.005, xlim=c(0.05, 0.5), data=sampledist.200)
dotPlot(~ prop, width=.005, xlim=c(0.05, 0.5), data=sampledist.50)
@

%Example 3.10 (do confidence intervals?)

\subsection*{Importance of Random Sampling}

%Example 3.11

\section{Understanding and Interpreting Confidence Intervals}

\subsection*{Interval Estimates and Margin of Error}

\begin{boxedText}
An \term{interval estimate} 
gives a range of plausible values for a population parameter.  
\end{boxedText}

This is better than a single number (also called a point estimate) because it gives some 
indication of the precision of the estimate.

One way to express an interval estimate is with a point estimate and a \term{margin of error}.

We can convert margin of error into an interval by adding and subtracting the margin of error to/from
the statistic.

\subsubsection*{Example 3.12}

\[
0.42 \pm 0.03 \mbox{ which is the same as } (0.39, 0.45)
\]

\subsubsection*{Example 3.13}

<<marginoferror, tidy=FALSE>>=
p.hat = 0.54                          # sample proportion
MoE = 0.02                            # margin of error
p.hat - MoE                         # lower limit of interval estimate
p.hat + MoE                         # upper limit of interval estimate
@

<<>>=
p.hat =0.54 
MoE=0.10
p.hat - MoE
p.hat + MoE
@

\subsection*{Confidence Intervals}

\begin{boxedText}
A confidence interval for a parameter is an interval computed from sample data
by a method that will capture the parameter for a specified proportion of all
samples
\end{boxedText}

\begin{enumerate}
	\item The probability of correctly containing the parameter is called the coverage rate or \term{confidence level}.
	\item So 95\% of 95\% confidence intervals contain the parameter being estimated.
	\item The margins of error in the tables above were designed to produce 95\% confidence intervals.
\end{enumerate}

\subsubsection*{Example 3.14}

<<confidenceinterval, tidy=FALSE>>=
x.bar = 61.5               # given sample mean
SE = 11                    # given estimated standard error
MoE = 2 * SE; MoE          # margin of error for 95% CI
x.bar - MoE                # lower limit of 95% CI
x.bar + MoE                # upper limit of 95% CI
@

\subsection*{Understanding Confidence Intervals}
\subsubsection*{Example 3.15}

<<confidenceinterval2>>=
SE=0.03
p1=0.26 
p2=0.32
p3=0.20
MoE=2*SE
@

<<>>=
p1-MoE
p1+MoE
p2-MoE
p2+MoE
p3-MoE
p3+MoE
@

<<>>=
p= 0.275
SE= 0.03
MoE = 2 * SE
p - MoE
p + MoE

dotPlot(~ prop, width=.005, groups = (0.215 <= prop & prop <= 0.335), data=sampledistdeg)
@

%how to do plots on page 184?

\subsection*{Interpreting Confidence Intervals}

\subsubsection*{Example 3.16}

<<>>=
x.bar= 27.655
SE= 0.009
MoE = 2 * SE
x.bar - MoE
x.bar + MoE
@

\subsubsection*{Example 3.17}

<<>>=
diff.x= -1.915
SE= 0.016
MoE = 2 * SE
diff.x - MoE
diff.x + MoE
@

\section{Constructing Bootstrap Confidence Intervals}

Here's the clever idea:  We don't have the population, but we have a sample.  Probably the sample it similar to the population in many ways.  So let's sample from our sample.  We'll call it \textbf{resampling} (also called \textbf{bootstrapping}). We want samples the same size as our original sample, so we will need to sample with replacement.  This means that we may pick some members of the population more than once and others not at all.  We'll do this many times, however, so each member of our sample will get its fair share. (Notice the similarity to and difference from sampling from populations in the previous sections.) 

\subsubsection*{Commuting in Atlanta}
<<fig.width=5>>=
head(CommuteAtlanta, 3)
dotPlot(~Time, width=1, cex=.5, data=CommuteAtlanta)
mean(~Time, data=CommuteAtlanta)
sd(~Time, data=CommuteAtlanta)
@

\subsection*{Bootstrap Samples}
The computer can easily do all of the resampling by using the \function{resample()}.
<<>>=
resample(CommuteAtlanta, 10)
@

\subsection*{Bootstrap Distribution}

The example below uses data from 500 Atlanta commuters.
<<>>=
mean( ~Time, data=resample(CommuteAtlanta) )  # mean commute time in one resample
mean( ~Time, data=resample(CommuteAtlanta) )  # mean commute time in another resample

# Now we'll do it 1000 times
Bootstrap <- do(1000) * mean( ~Time, data=resample(CommuteAtlanta)) 
@

We should check that that our bootstrap distribution has an appropriate shape:
<<warning=FALSE>>=
histogram(~result, data=Bootstrap, n=30)
dotPlot(~result, data=Bootstrap, n=50)
@

\subsubsection*{Example 3.19}
<<fig.width=5>>=
BootP <- do(1000) * rflip(100, .52)
head(BootP, 3)
dotPlot(~ prop, width=.01, data=BootP)
@

%\subsubsection*{Example 3.20}
% <<>>=
% 
% @
% make dataset?

\subsection*{Estimating Standard Error Based on a Bootstrap Distribution}
\subsubsection*{Example 3.21}
Since the shape of the bootstrap distribution from Example 3.19 looks good, we can estimate the standard error.
<<>>=
SE = sd(~prop, data=BootP); SE
@

\subsubsection*{Example 3.22}
We can again use the standard error to compute a 95\% confidence interval.
<<tidy=FALSE>>=
x.bar <- mean(~Time, data = CommuteAtlanta); x.bar
SE <- sd(~result, data = Bootstrap ); SE        # standard error
MoE <- 2 * SE; MoE                              # margin of error for 95% CI
x.bar - MoE                                     # lower limit of 95% CI
x.bar + MoE                                     # upper limit of 95% CI
@

<<>>=
p.hat = 0.52
SE = sd( ~result, data=Bootstrap ); SE      
MoE = 2 * SE; MoE                          
p.hat - MoE                                 
p.hat + MoE                                 
@


The same steps used in this example, get used in a wide variety of confidence interval situations.
\begin{enumerate}
	\item 
		Compute the statistic from the original sample.
	\item
		Create a bootstrap distribution by resampling from the sample.
		\begin{enumerate}
			\item same size samples as the original sample
			\item with replacement
			\item compute the statistic for each sample
		\end{enumerate}
		The distribution of these statistics is the bootstrap distribution
	\item
		Estimate the standard error $SE$ by computing the standard deviation of the bootstrap distribution.
	\item
		95\% CI is \[ \mbox{statistic} \pm 2 SE \]
\end{enumerate}




\section{More Examples}

This section contains the R code for some additional examples (with minimal discussion)

\subsection{Comparing two proportions}

<<tidy=FALSE>>=
tally( play ~ playVer, data=littleSurvey )
prop( play ~ playVer, data=littleSurvey )
stat <- diff( prop( play ~ playVer, data=littleSurvey ) ); stat
Boot.Play <- do(1000) * diff(prop( play ~ playVer, data=resample(littleSurvey) ))
head(Boot.Play)
histogram(~ no.v2, data=Boot.Play)
@
<<>>=
SE <- sd( ~ no.v2, data=Boot.Play ); SE
stat - 2 * SE
stat + 2 * SE
cdata( .95, no.v2, data=Boot.Play )
@
Again, the two confidence intervals are similar.

Notice importantly that 0 is not in these intervals.  That means all the plausible values
for the difference between the proportions have the version 2 respondents more likely to 
\emph{not} go to the play.

\subsection{Comparing two means}
We can compare two means in a similar way.  Let's estimate the difference 
between mean pulse for men and women (based on a sample of students in an introductory
statistics course at another institution).

<<tidy=FALSE>>=
mean( Pulse ~ Sex, data=StudentSurvey )
stat <- diff(mean(  Pulse ~ Sex, data=StudentSurvey )); stat
Boot.Pulse <- do(1000) * diff(mean(  Pulse ~ Sex, data=resample(StudentSurvey) ))
head(Boot.Pulse, 3)
histogram( ~ Male, data=Boot.Pulse )
@
<<>>=
SE <- sd( ~ Male, data=Boot.Pulse ); SE
stat - 2 * SE
stat + 2 * SE
cdata( .95, Male, data=Boot.Pulse )
@

\subsection{One proportion -- from data}

What percent of students smoke?

<<>>=
stat <- prop( ~ Smoke, data=StudentSurvey ); stat
Boot.Smoke <- do(1000) * prop( ~ Smoke, data=resample(StudentSurvey) )
head(Boot.Smoke, 3)
histogram( ~No, data=Boot.Smoke )
@
<<>>=
SE <- sd( ~No, data=Boot.Smoke )
stat - SE
stat + SE
cdata( .95, No, data=Boot.Smoke )
@

\subsection{One proportion -- from a data summary}

Here is a report from a recent USA Today poll:
\begin{center}
	\includegraphics[width=.7\textwidth]{images/ObamaCarePoll}
\end{center}

Let's compute a 95\% confidence interval for the proportion of people
disapprove of the way Obama is handling health care policy.
This time we don't have data, but we can still simulate the bootstrap distribution --
this time we will use \function{rflip()}.

<<>>=
stat <- 0.53
Boot.Poll <- do(1000) * rflip(1503, .53)
head(Boot.Poll, 3)
histogram(~ prop, data=Boot.Poll)
@
<<>>=
SE <- sd(~prop, data=Boot.Poll); SE
2 * SE             # margin of error
stat - 2 * SE      # lower end of interval
stat + 2 * SE      # upper end of interval
cdata( .95, prop, data=Boot.Poll) # quantile method
@
These results are consistent with USA Today's claim that the margin of error is $\pm 3\%$.

\subsection{A non-example: Estimating the median from a small sample}

<<>>=
median( ~ Price, data=MustangPrice )
Boot.Mustang <- do( 1000 ) * median( ~Price, data=resample(MustangPrice) )
head(Boot.Mustang, 3)
histogram( ~ result, data=Boot.Mustang, n=50 )
@

This time the histogram does not have the desired shape.  There are two problems:
\begin{enumerate}
	\item
		The distribution is not symmetric.  (It is right skewed.)
	\item
		The distribution has spikes and gaps.

Since the median must be
an element of the sample when the sample size is 25, there are only 25 possible values for the median (and some of 
these are \emph{very} unlikely.
\end{enumerate}
Since the bootstrap distribution does not look like a normal distribution (bell-shaped, symmetric), we cannot safely use 
our methods for creating a confidence interval.

%%%%%%%%%%%%%%%%%%%%
\subsection{Using the standard error to compute a margin of error}

In many (but not all) situations, the sampling distribution is 
\begin{itemize}
  \item unimodal,
	\item symmetric, and
	\item bell-shaped
\end{itemize}
(The technical phrase is ``approximately normal''.)  In these situations, a 95\% confidence interval can be estimated 
with
\[ 
\mbox{statistic} \pm 2 SE
\]

\textbf{Example.}  A large sample of American adults the mean body mass index was $27.655$.  The standard 
error was $0.009$. The 95\% confidence interval is therefore 
\[
27.655 \pm 0.018
\]


The important question remains:  If we don't have access to the entire population, and we only get to look at one sample,
how do we tell the shape of the sampling distribution and estimate its standard error?  Stay tuned.

Another way to create a 95\% confidence interval is to use the middle 95\% of the bootstrap distribution.
The \function{cdata()} function can compute this for us as follows:
<<>>=
cdata(0.95, prop, data=sampledistdeg)
dotPlot(~ prop, width=.005, groups = (0.215 <= prop & prop <= 0.34), data=sampledistdeg)
@
This is not exactly the same as the interval above, but it is pretty close.

One advantage of this method is that it is easy to change the confidence level.  To make a 90\% confidence interval,
we use the middle 90\% of the sample distribution instead.
<<>>=
cdata(0.90, prop, data=sampledistdeg)
@
Notice that this interval is narrower.  This will always be the case.  Higher levels of confidence 
lead to wider confidence intervals.

(Method 1 can also be adjusted for other confidence levels as well -- 
the number 2 needs to be replaced by an appropriate alternative.)

<<>>=
cdata(0.95, prop, data=(do(1000) * rflip(200, 0.275)))
@